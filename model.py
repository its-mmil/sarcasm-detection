import tensorflow as tf

class Model(tf.keras.Model):

    def __init__(self, vocab_size):
        super().__init__()

        embed_size = 64
        rnn_size = 128
        self.embedding1 = tf.keras.layers.Embedding(vocab_size, embed_size)
        self.embedding2 = tf.keras.layers.Embedding(vocab_size, embed_size)
        self.embedding3 = tf.keras.layers.Embedding(vocab_size, embed_size)
        self.gru1 = tf.keras.layers.GRU(rnn_size, return_sequences = True)
        self.gru2 = tf.keras.layers.GRU(rnn_size, return_sequences = True)
        self.gru3 = tf.keras.layers.GRU(rnn_size, return_sequences = True)
        self.flatten = tf.keras.layers.Flatten()
        self.dropout = tf.keras.layers.Dropout(0.5)
        self.softmax_dense1 = tf.keras.layers.Dense(2, activation="softmax", name='softmax_dense1') # TODO: Names for debugging.
        self.softmax_dense2 = tf.keras.layers.Dense(2, activation="softmax")
        self.softmax_dense3 = tf.keras.layers.Dense(2, activation="softmax")
        self.relu_dense1 = tf.keras.layers.Dense(2, activation=tf.keras.layers.LeakyReLU(alpha=0.3), name='relu_dense1')
        self.relu_dense2 = tf.keras.layers.Dense(2, activation=tf.keras.layers.LeakyReLU(alpha=0.3), name='relu_dense2')

    def call(self, inputs, training=True): # Inputs in format (all text, sentiment words, non-sentiment words)
        sent_words = inputs[0]
        non_sent_words = inputs[1]
        all_text = inputs[2]

        embedded_sent = self.embedding1(sent_words)
        embedded_non_sent = self.embedding2(non_sent_words)
        embedded_all = self.embedding3(all_text)

        # Dropout if training
        embedded_sent = self.dropout(embedded_sent, training)
        embedded_non_sent = self.dropout(embedded_non_sent, training)
        embedded_all = self.dropout(embedded_all, training)

        encoded_sent = self.flatten(self.gru1(embedded_sent))
        encoded_non_sent = self.flatten(self.gru2(embedded_non_sent))
        encoded_all = self.flatten(self.gru3(embedded_all))
        
        # Dropout if training # (dropout not used here in paper implementation)
        # encoded_sent = self.dropout(encoded_sent, training)
        # encoded_non_sent = self.dropout(encoded_non_sent, training)
        # encoded_all = self.dropout(encoded_all, training)

        lit_pred = self.softmax_dense1(encoded_sent)
        imp_pred = self.softmax_dense2(encoded_non_sent) # Literal and implied channel sentiment preds generated by running dense layer on encoded sentiment and non-sentiment words

        combined_lit_all = tf.concat([encoded_sent, encoded_all], axis = 1)
        combined_imp_all = tf.concat([encoded_non_sent, encoded_all], axis = 1)
        combined_lit_all = self.relu_dense1(combined_lit_all)
        combined_imp_all = self.relu_dense2(combined_imp_all)
        combined_channels = tf.concat([combined_lit_all, combined_imp_all], axis = 1)
        sarc_pred = self.softmax_dense3(combined_channels) # Sarcasm prediction generated running dense layer on the combination of both channels' outputs.

        return (lit_pred, imp_pred, sarc_pred)
